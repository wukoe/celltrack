{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process data\n",
    "import os,gc\n",
    "from importlib import reload\n",
    "import torch\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from model_args import aot_args,sam_args,segtracker_args\n",
    "\n",
    "from wbtool.show import wshow\n",
    "from wbtool import file_misc as fm\n",
    "from wbtool import use_models\n",
    "import proc_data\n",
    "\n",
    "# vid = np.load('/home/wb/samba_dir/cells_brightfield/leadingcell-WB-entirespan/B1ROI1.npy')\n",
    "# vid = np.load('/home/wb/samba_dir/cells_brightfield/leading-HELA-entirespan/A1ROI1.npy')\n",
    "vid = np.load('/home/wb/samba_dir/cells_brightfield/single-cell-movement-for-machine-learning/A549/0/A4ROI1.npy')\n",
    "# vid = vid[:7]\n",
    "\n",
    "vid = proc_data.value_modify(vid)\n",
    "vid = proc_data.ch_num_modify(vid)\n",
    "# print(vid.shape)\n",
    "\n",
    "# ids,coords = proc_data.read_coords_from_csv('/home/wb/samba_dir/cells_brightfield/leadingcell-WB-entirespan/Results_B1ROI1.csv')\n",
    "# ids,coords = proc_data.read_coords_from_csv('/home/wb/samba_dir/cells_brightfield/leading-HELA-entirespan/Results_A1ROI1.csv')\n",
    "ids,coords = proc_data.read_coords_from_csv('/home/wb/samba_dir/cells_brightfield/single-cell-movement-for-machine-learning/A549/0/Results_A4ROI1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wbtool import show \n",
    "\n",
    "fid = 18\n",
    "point_prompt = coords[fid]\n",
    "obj_mode = np.ones((len(point_prompt), 1))\n",
    "frame = vid[fid]\n",
    "frame = proc_data.value_modify(frame)\n",
    "print(frame.shape)\n",
    "\n",
    "show.overlay_point(frame, point_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sam_checkpoint': '/home/wb/wb_data/model_parameter_storage/SAM/sam_vit_h_4b8939.pth', 'model_type': 'vit_h', 'generator_args': {'points_per_side': 30, 'pred_iou_thresh': 0.8, 'stability_score_thresh': 0.9, 'crop_n_layers': 1, 'crop_n_points_downscale_factor': 2, 'min_mask_region_area': 200}, 'gpu_id': 0}\n"
     ]
    }
   ],
   "source": [
    "import SegTracker\n",
    "from model_args import aot_args,sam_args,segtracker_args\n",
    "# importlib.reload(SegTracker)\n",
    "print(sam_args)\n",
    "sam_args['generator_args'] = {\n",
    "        'points_per_side': 30,\n",
    "        'pred_iou_thresh': 0.8,\n",
    "        'stability_score_thresh': 0.9,\n",
    "        'crop_n_layers': 1,\n",
    "        'crop_n_points_downscale_factor': 2,\n",
    "        'min_mask_region_area': 200,\n",
    "    }\n",
    "\n",
    "# segtracker = SegTracker.SegTracker(segtracker_args,sam_args,aot_args)\n",
    "SA = use_models.cv.sam_agent('prompt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment for frame 30\n",
      "-segmentation done\n",
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from /home/wb/tools/Segment-and-Track-Anything/ckpt/groundingdino_swint_ogc.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "SegTracker has been initialized\n",
      "processed frame 30, obj_num 71\n",
      "-forward tracking finished\n"
     ]
    }
   ],
   "source": [
    "### Generate Results for the Whole Video\n",
    "from skimage import morphology\n",
    "from onevision import morphology as ovmorph\n",
    "from onevision import morph_proc\n",
    "import aux\n",
    "# reload(aux)\n",
    "# reload(proc_data)\n",
    "# reload(morph_proc)\n",
    "# reload(ovmorph)\n",
    "\n",
    "segtracker_args = {\n",
    "    'sam_gap': 1, # the interval to run sam to segment new objects\n",
    "    'min_area': 20, # minimal mask area to add a new mask as a new object\n",
    "    'max_obj_num': 255, # maximal object number to track in a video\n",
    "    'min_new_obj_iou': 0.8, # the area of a new object in the background should > 80% \n",
    "    'new_replace_iou_min': 0.5,\n",
    "    'remain_remove_max':0.4,\n",
    "    'min_obj_area_ratio':0.0001,\n",
    "    'max_obj_area_ratio':0.01\n",
    "}\n",
    "\n",
    "# output masks\n",
    "output_dir = '/home/wb/samba_dir/indev/results' #io_args['output_mask_dir']\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# grounding_caption = \"cell\"\n",
    "# box_threshold, text_threshold, box_size_threshold, reset_image = 0.02, 0.02, 0.05, True\n",
    "\n",
    "# sz = vid[0].shape\n",
    "# frame_area = sz[0] * sz[1]\n",
    "\n",
    "# === get segmentation results of each frame.\n",
    "# (this is independent of tracking activity)\n",
    "# seg_list = fm.pload(os.path.join(output_dir,'../sms2.pk'))\n",
    "seg_list = aux.do_segment(SA, segtracker_args, vid, ids, coords)\n",
    "fm.pdump(os.path.join(output_dir,'../sms2.pk'), seg_list)\n",
    "\n",
    "# === tracking\n",
    "segtracker = SegTracker.SegTracker(segtracker_args, sam_args, aot_args)\n",
    "segtracker.restart_tracker()   # 调用的是aot_tracker/AOTTracker function, AOT模型自身的restart要求.\n",
    "fnum = len(vid)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "pred_list = []; track_list = []\n",
    "# with torch.cuda.amp.autocast():\n",
    "for frame_idx in range(fnum):\n",
    "    frame = vid[frame_idx]\n",
    "    track_list.append(None)\n",
    "    if frame_idx == 0:\n",
    "        # print(segtracker.curr_idx)\n",
    "        pred_mask = morph_proc.imbind_to_map(seg_list[frame_idx])\n",
    "        segtracker.add_reference(frame, pred_mask)\n",
    "    elif (frame_idx % segtracker_args['sam_gap']) == 0:\n",
    "        # 1/2\n",
    "        seg_mask = seg_list[frame_idx]\n",
    "        # 2/2\n",
    "        # seg_mask,_ = segtracker.detect_and_seg(frame, grounding_caption, box_threshold, text_threshold, box_size_threshold, reset_image)\n",
    "        # e/2\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "        # gc.collect()\n",
    "        track_mask = segtracker.track(frame)\n",
    "        track_mask = ovmorph.IMbind(track_mask, 'map')\n",
    "        track_mask = aux.isolate_filter_object(track_mask, segtracker_args)\n",
    "        track_list[frame_idx] = track_mask        \n",
    "        pred_mask = aux.merge_st(segtracker_args, track_mask, seg_mask)\n",
    "\n",
    "        # segtracker.restart_tracker()\n",
    "        segtracker.add_reference(frame, pred_mask)\n",
    "    else:\n",
    "        pred_mask = segtracker.track(frame, update_memory=True)\n",
    "    pred_list.append(pred_mask)\n",
    "    # gc.collect()\n",
    "    proc_data.save_prediction(pred_mask, output_dir, str(frame_idx)+'.png')    \n",
    "    print(\"processed frame {}, obj_num {}\".format(frame_idx, segtracker.get_obj_num()), end='\\r')\n",
    "print('\\n-forward tracking finished')\n",
    "# fm.pdump(os.path.join(output_dir,'../tms.pk'), track_list)\n",
    "# fm.pdump(os.path.join(output_dir,'../pms.pk'), pred_list)\n",
    "\n",
    "# == backward merge part.\n",
    "# for frame_idx in range(fnum-2, -1, -1):\n",
    "#     print(frame_idx)\n",
    "#     frame = vid[frame_idx]\n",
    "\n",
    "#     if (frame_idx % segtracker_args['sam_gap']) == 0:\n",
    "#         # 1/2\n",
    "#         seg_mask = pred_list[frame_idx]\n",
    "#         # 2/2\n",
    "#         # seg_mask,_ = segtracker.detect_and_seg(frame, grounding_caption, box_threshold, text_threshold, box_size_threshold, reset_image)\n",
    "#         # e/2\n",
    "\n",
    "#         # torch.cuda.empty_cache()\n",
    "#         # gc.collect()\n",
    "#         track_mask = segtracker.track(frame)\n",
    "#         track_mask = aux.isolate_filter_object(track_mask, segtracker_args)\n",
    "#         track_list[frame_idx] = track_mask\n",
    "#         pred_mask = aux.merge_st(segtracker_args, track_mask, seg_mask)\n",
    "\n",
    "#         # segtracker.restart_tracker()\n",
    "#         segtracker.add_reference(frame, pred_mask)\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "#     proc_data.save_prediction(pred_mask, output_dir, str(frame_idx)+'.png')    \n",
    "#     print(\"processed frame {}, obj_num {}\".format(frame_idx, segtracker.get_obj_num()), end='\\r')\n",
    "# print('\\n-backward tracking finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43]\n"
     ]
    }
   ],
   "source": [
    "print(ids[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "供参考的SegTracker函数\n",
    "\n",
    "刘平版：\n",
    "def detect_and_seg_pt(self, origin_frame: np.ndarray, coords, modes):\n",
    "    '''\n",
    "    Using Grounding-DINO to detect object acc Text-prompts\n",
    "    Retrun:\n",
    "        refined_merged_mask: numpy array (h, w)\n",
    "        annotated_frame: numpy array (h, w, 3)\n",
    "    '''\n",
    "            # backup id and origin-merged-mask\n",
    "    bc_id = self.curr_idx\n",
    "    bc_mask = self.origin_merged_mask\n",
    "    \n",
    "    # get annotated_frame and boxes\n",
    "    # annotated_frame, boxes = self.detector.run_grounding(origin_frame, grounding_caption, box_threshold, text_threshold)\n",
    "    for i in range(len(coords)):\n",
    "        pt = coords[i].reshape([1,2])\n",
    "        mode = modes[i]\n",
    "        # if (bbox[1][0] - bbox[0][0]) * (bbox[1][1] - bbox[0][1]) > annotated_frame.shape[0] * annotated_frame.shape[1] * box_size_threshold:\n",
    "        #     continue\n",
    "        interactive_mask = self.sam.segment_with_click(origin_frame, pt, mode)\n",
    "\n",
    "        mask = interactive_mask.copy()\n",
    "        if(mask.sum(1).sum(0)/(mask.shape[0]*mask.shape[1]) < 0.01): \n",
    "            refined_merged_mask = self.add_mask(interactive_mask)\n",
    "            self.update_origin_merged_mask(refined_merged_mask)\n",
    "            self.curr_idx += 1\n",
    "\n",
    "    # reset origin_mask\n",
    "    self.reset_origin_merged_mask(bc_mask, bc_id)\n",
    "\n",
    "    return refined_merged_mask\n",
    "\n",
    "最终版：\n",
    "    def detect_and_seg_pt(self, frame: np.ndarray, coords, modes):\n",
    "        '''\n",
    "        Using Grounding-DINO to detect object acc Text-prompts\n",
    "        Retrun:\n",
    "            refined_merged_mask: numpy array (h, w)\n",
    "            annotated_frame: numpy array (h, w, 3)\n",
    "        '''\n",
    "        # backup id and origin-merged-mask\n",
    "        bc_id = self.curr_idx\n",
    "        bc_mask = self.origin_merged_mask\n",
    "    \n",
    "        # get annotated_frame and boxes\n",
    "        # annotated_frame, boxes = self.detector.run_grounding(frame, grounding_caption, box_threshold, text_threshold)\n",
    "        cnum = len(coords)\n",
    "        imgarea = frame.shape[0]*frame.shape[1]\n",
    "        # ims = []\n",
    "        refined_merged_mask = self.add_mask(np.zeros(frame.shape[:2], dtype=np.uint8))\n",
    "        for k in range(cnum):\n",
    "            # if (bbox[1][0] - bbox[0][0]) * (bbox[1][1] - bbox[0][1]) > annotated_frame.shape[0] * annotated_frame.shape[1] * box_size_threshold:\n",
    "            #     continue\n",
    "            interactive_mask = self.sam.segment_with_click(frame, coords[k:k+1], modes[k], True)\n",
    "            # ims.append(interactive_mask.copy())\n",
    "            \n",
    "            if interactive_mask.sum(1).sum(0)/imgarea < 0.01:\n",
    "                refined_merged_mask = self.add_mask(interactive_mask)  #在self.origin_merged_mask 的基础上，根据输入修改得到返回值。\n",
    "                self.update_origin_merged_mask(refined_merged_mask)\n",
    "                self.curr_idx += 1\n",
    "\n",
    "        # reset origin_mask\n",
    "        self.reset_origin_merged_mask(bc_mask, bc_id)\n",
    "\n",
    "        return refined_merged_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
