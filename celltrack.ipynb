{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process data\n",
    "import os,gc,glob\n",
    "# import sys\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from wbtool.show import wshow\n",
    "from wbtool import file_misc as fm\n",
    "import proc_data,aux\n",
    "\n",
    "video_dir = '/home/wb/samba_dir/cells_brightfield/single-cell-movement-for-machine-learning/WB-F344'\n",
    "# video_dir = '/home/wb/samba_dir/cells_brightfield/single-cell-movement-for-machine-learning/A549/'\n",
    "# video_dir = '/home/wb/samba_dir/cells_brightfield/single-cell-movement-for-machine-learning/HELA/'\n",
    "# '/home/wb/samba_dir/cells_brightfield/leadingcell-WB-entirespan/B1ROI1.npy'\n",
    "# '/home/wb/samba_dir/cells_brightfield/leading-HELA-entirespan/A1ROI1.npy'\n",
    "# vid = np.load('/home/wb/samba_dir/cells_brightfield/single-cell-movement-for-machine-learning/A549/0/A4ROI1.npy')\n",
    "# vid = np.load('/home/wb/samba_dir/cells_brightfield/temp/A4ROI1.npy')  # difficult\n",
    "annotation_dir = video_dir\n",
    "cell_mask_dir = '/home/wb/samba_dir/cells_brightfield/model0808_mask/mask_WB-F344/' \n",
    "# cell_mask_dir = '/home/wb/samba_dir/cells_brightfield/model0808_mask/mask_A549/' \n",
    "\n",
    "# output masks\n",
    "# output_dir = '/home/wb/samba_dir/indev/results' #io_args['output_mask_dir']\n",
    "output_dir = '/home/wb/samba_dir/cells_brightfield/model0808_mask_trackres/WB-F344/' \n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load video frames and point coordinates.\n",
    "'''\n",
    "# reload(proc_data)\n",
    "dscode = 'A4ROI3'\n",
    "fcode = 0\n",
    "\n",
    "# vid = np.load(os.path.join(video_dir, '{}/{}.npy'.format(fcode, dscode)))\n",
    "# # vid = vid[:5]\n",
    "# vid = proc_data.value_modify(vid)\n",
    "# vid = proc_data.ch_num_modify(vid)\n",
    "# print(vid.dtype)\n",
    "\n",
    "ids,coords = proc_data.read_coords_from_csv(os.path.join(annotation_dir, '{}/Results_{}.csv'.format(fcode, dscode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show one frame's image and point location. 用于确认载入的图像和point是否正确。\n",
    "from wbtool import show \n",
    "\n",
    "fid = 1\n",
    "# vid = seg_series\n",
    "point_prompt = coords[fid] #[:,[1,0]]\n",
    "# print(point_prompt)\n",
    "obj_mode = np.ones((len(point_prompt), 1))\n",
    "frame = vid[fid]\n",
    "frame = proc_data.value_modify(frame)\n",
    "print(frame.shape)\n",
    "\n",
    "show.overlay_point(frame, point_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' to get models and tracker needed for segmentation/tracking\n",
    "'''\n",
    "# import SegTracker\n",
    "# from wbtool import use_models\n",
    "# from model_args import aot_args,sam_args #,segtracker_args\n",
    "\n",
    "# print(sam_args)\n",
    "# sam_args['generator_args'] = {\n",
    "#         'points_per_side': 30,\n",
    "#         'pred_iou_thresh': 0.8,\n",
    "#         'stability_score_thresh': 0.9,\n",
    "#         'crop_n_layers': 1,\n",
    "#         'crop_n_points_downscale_factor': 2,\n",
    "#         'min_mask_region_area': 200,\n",
    "#     }\n",
    "\n",
    "segtracker_args = {\n",
    "    'sam_gap': 1, # the interval to run sam to segment new objects\n",
    "    'min_area': 20, # minimal mask area to add a new mask as a new object\n",
    "    'max_obj_num': 255, # maximal object number to track in a video\n",
    "    'min_new_obj_iou': 0.8, # the area of a new object in the background should > 80% \n",
    "    'new_replace_iou_min': 0.5,\n",
    "    'remain_remove_max':0.4,\n",
    "    'min_obj_area_ratio':0.0001,\n",
    "    'max_obj_area_ratio':0.01\n",
    "}\n",
    "\n",
    "# tool models.\n",
    "SA = use_models.cv.sam_agent('prompt')\n",
    "segtracker = SegTracker.SegTracker(segtracker_args, sam_args, aot_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get segmentation mask.\n",
    "'''\n",
    "# === get segmentation results of each frame.\n",
    "# (this is independent of tracking activity)\n",
    "# reload(aux)\n",
    "use_saved = True\n",
    "# file = os.path.join(output_dir,'../sms.pk')\n",
    "\n",
    "if use_saved:\n",
    "    # seg_series = fm.pload(file)\n",
    "    # load multiple figures\n",
    "    flist = glob.glob(os.path.join(cell_mask_dir, '{}/{}/*.png'.format(fcode, dscode)))\n",
    "    flist.sort()\n",
    "    print(len(flist))\n",
    "    seg_series = []\n",
    "    for file in flist:\n",
    "        seg_series.append(fm.imread(file))\n",
    "    # seg_series: 31 of [(904, 1224)] with dtype int32.\n",
    "else:    \n",
    "    seg_series = aux.do_segment(SA, segtracker_args, vid, ids, coords)\n",
    "    fm.pdump(file, seg_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generate Results for the Whole segmented series (不使用SegTracker，完全基于已分割的frame串起来的版本)\n",
    "'''\n",
    "from onevision import morphology as ovmorph\n",
    "from onevision import morph_data\n",
    "reload(morph_data)\n",
    "use_point = False\n",
    "\n",
    "fnum = len(seg_series)\n",
    "\n",
    "pred_list = []; track_list = []\n",
    "\n",
    "def link_map(M1, M2):\n",
    "    ''' 在两个ID信息无关的map之间建立细胞间的对应关系。\n",
    "    '''\n",
    "    L = []\n",
    "    for idx in M1:\n",
    "        obj = M1[idx]\n",
    "        tid = ovmorph.find_overlap(obj, M2, 'source')\n",
    "        L.append([idx, tid])\n",
    "    return L\n",
    "\n",
    "# wshow(seg_series[1])\n",
    "# wshow(seg_series[0])\n",
    "link_L = []\n",
    "for frame_idx in range(fnum-1):\n",
    "    a = link_map(morph_data.IMbind(seg_series[frame_idx], 'map'), morph_data.IMbind(seg_series[frame_idx + 1], 'map'))\n",
    "    link_L.append(a)\n",
    "    # print(\"processed frame {}\".format(frame_idx), end='\\r')\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: [2], 7: [4], 8: [5], 10: [22], 35: [21], 40: [27]}\n",
      "{}\n",
      ">> start single missing fix\n",
      "{3: [2], 7: [4], 8: [5], 10: [22], 35: [21], 40: [27]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "根据人工点的信息给对应的细胞mask设定id。\n",
    "（不被点覆盖的mask忽略）\n",
    "'''\n",
    "# reload(aux)\n",
    "new_series = aux.connect_id_mask(seg_series, ids, coords)\n",
    "\n",
    "'''\n",
    "post-processing \n",
    "'''\n",
    "# reload(aux)\n",
    "# === fix frame's masks.\n",
    "for k in range(len(new_series)):\n",
    "    new_series[k] = aux.fix_cell_mask(new_series[k], False, False)\n",
    "    print('done', k, end='\\r')\n",
    "\n",
    "# === fix sequence.\n",
    "from onevision import trajectory\n",
    "nsms = aux.fix_trace(new_series, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wb/samba_dir/cells_brightfield/model0808_mask_trackres/WB-F344/0/\n"
     ]
    }
   ],
   "source": [
    "# 保存细胞轨迹结果（以每帧的分割的结果的形式）\n",
    "folder = os.path.join(output_dir, '{}/'.format(fcode))\n",
    "print(folder)\n",
    "# from misc_tool import show\n",
    "# reload(show)\n",
    "tp = os.path.join(folder, dscode)\n",
    "\n",
    "# === 染色\n",
    "# pm = proc_data.overlay_all(vid, nsms, show_id=True)\n",
    "# proc_data.all2imgfile(pm, tp, dscode+'_', direct_write=True)\n",
    "\n",
    "for k in range(len(nsms)):\n",
    "    fm.pdump(os.path.join(tp, dscode+'_{}.pk'.format(k)), nsms[k].DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generate Results for the Whole Video (基于SegTracker的版本)\n",
    "'''\n",
    "# importlib.reload(SegTracker)\n",
    "# from skimage import morphology\n",
    "# from onevision import morphology as ovmorph\n",
    "from onevision import morph_data\n",
    "# from wbtool import show\n",
    "# reload(aux)\n",
    "# reload(proc_data)\n",
    "# reload(morph_data)\n",
    "# reload(ovmorph)\n",
    "\n",
    "# grounding_caption = \"cell\"\n",
    "# box_threshold, text_threshold, box_size_threshold, reset_image = 0.02, 0.02, 0.05, True\n",
    "\n",
    "# === tracking\n",
    "flag_do_backward = False\n",
    "# segtracker = SegTracker.SegTracker(segtracker_args, sam_args, aot_args)\n",
    "segtracker.restart_tracker()   # 调用的是aot_tracker/AOTTracker function, AOT模型自身的restart要求.\n",
    "fnum = len(vid)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "pred_list = []; track_list = []; #sm2=[]; sm1=[]\n",
    "# with torch.cuda.amp.autocast():\n",
    "for frame_idx in range(fnum):\n",
    "    frame = vid[frame_idx]\n",
    "    track_list.append(None)\n",
    "    if frame_idx == 0:\n",
    "        # print(segtracker.curr_idx)\n",
    "        pred_mask = morph_data.imbind_to_map(seg_series[frame_idx])\n",
    "        segtracker.add_reference(frame, pred_mask)\n",
    "    elif (frame_idx % segtracker_args['sam_gap']) == 0:\n",
    "        # 1/2\n",
    "        seg_mask = seg_series[frame_idx]\n",
    "        # 2/2\n",
    "        # seg_mask,_ = segtracker.detect_and_seg(frame, grounding_caption, box_threshold, text_threshold, box_size_threshold, reset_image)\n",
    "        # e/2\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "        # gc.collect()\n",
    "        track_mask = segtracker.track(frame)\n",
    "        \n",
    "        track_mask = morph_data.IMbind(track_mask, 'map')\n",
    "        # print('track output', track_mask.ids)\n",
    "        track_mask = aux.isolate_filter_object(track_mask, segtracker_args)\n",
    "        track_list[frame_idx] = track_mask\n",
    "        # print('k7') #('before st ', track_mask.ids)\n",
    "        pred_mask = aux.merge_st(segtracker_args, track_mask, seg_mask)\n",
    "        # print('k8') #('after st ', pred_mask)\n",
    "        pred_mask = morph_data.imbind_to_map(pred_mask)\n",
    "        # print('k10') #('merged ', list(np.unique(pred_mask)))\n",
    "        \n",
    "        # segtracker.restart_tracker()\n",
    "        segtracker.add_reference(frame, pred_mask)\n",
    "    else:\n",
    "        pred_mask = segtracker.track(frame, update_memory=True)\n",
    "    pred_list.append(pred_mask)\n",
    "    # gc.collect()\n",
    "    proc_data.save_prediction(pred_mask, output_dir, str(frame_idx)+'.png', opt='overlay', image=frame)    \n",
    "    print(\"processed frame {}, obj_num {}\".format(frame_idx, segtracker.get_obj_num()), end='\\r')\n",
    "print('\\n-forward tracking finished')\n",
    "fm.pdump(os.path.join(output_dir,'../tms.pk'), track_list)\n",
    "fm.pdump(os.path.join(output_dir,'../pms.pk'), pred_list)\n",
    "\n",
    "# == backward merge part.\n",
    "if flag_do_backward:\n",
    "    for frame_idx in range(fnum-2, -1, -1):\n",
    "        print(frame_idx)\n",
    "        frame = vid[frame_idx]\n",
    "\n",
    "        if (frame_idx % segtracker_args['sam_gap']) == 0:\n",
    "            # 1/2\n",
    "            seg_mask = pred_list[frame_idx]\n",
    "            # 2/2\n",
    "            # seg_mask,_ = segtracker.detect_and_seg(frame, grounding_caption, box_threshold, text_threshold, box_size_threshold, reset_image)\n",
    "            # e/2\n",
    "\n",
    "            # torch.cuda.empty_cache()l\n",
    "            # gc.collect()\n",
    "            track_mask = segtracker.track(frame)\n",
    "            track_mask = morph_data.IMbind(track_mask, 'map')\n",
    "            track_mask = aux.isolate_filter_object(track_mask, segtracker_args)\n",
    "            # track_list[frame_idx] = track_mask\n",
    "            pred_mask = aux.merge_st(segtracker_args, track_mask, seg_mask)\n",
    "            pred_mask = morph_data.imbind_to_map(pred_mask)\n",
    "\n",
    "            # segtracker.restart_tracker()\n",
    "            segtracker.add_reference(frame, pred_mask)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        proc_data.save_prediction(pred_mask, output_dir, str(frame_idx)+'.png', opt='overlay', image=frame)    \n",
    "        print(\"processed frame {}, obj_num {}\".format(frame_idx, segtracker.get_obj_num()), end='\\r')\n",
    "    print('\\n-backward tracking finished')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
